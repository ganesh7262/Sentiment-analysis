{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DATA PREPROCESSING</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>EDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as  plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset=pd.read_csv('final_dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw_list=stopwords.words('english')\n",
    "word_count={\n",
    "    \"positive\":[],\n",
    "    \"negative\":[],\n",
    "    \"neutral\":[],\n",
    "    \"irrelevant\":[]\n",
    "}\n",
    "\n",
    "pattern=re.compile(\"[^\\w ]\")\n",
    "\n",
    "for text,target in zip(final_dataset.text,final_dataset.sentiment):\n",
    "    text=re.sub(pattern,\"\",text).lower().split()\n",
    "    text=[word for word in text if word not in sw_list]\n",
    "    word_count[target].extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20,10.5))\n",
    "for axis, (target, words) in zip(axes.flatten(), word_count.items()):\n",
    "    bar_info = pd.Series(words).value_counts()[:25]\n",
    "    sns.barplot(x=bar_info.values, y=bar_info.index, ax=axis)\n",
    "    axis.set_title(f'Top words for {target}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PREPROCESSING</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en import English\n",
    "import emoji\n",
    "import spacy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, stopwords=stopwords):\n",
    "        self.vectorizer = TfidfVectorizer(lowercase=False, max_features=8000,\n",
    "                                         min_df=10, ngram_range=(1, 3),\n",
    "                                         tokenizer=None)\n",
    "        self.stopwords = stopwords\n",
    "        self.vectorizer_fitted = False\n",
    "        \n",
    "    def remove_urls(self, texts):\n",
    "        print('Removing URLs...')\n",
    "        pattern = re.compile('(\\w+\\.com ?/ ?.+)|(http\\S+)')\n",
    "        return [re.sub(pattern, '', text) for text in texts]\n",
    "    \n",
    "    def remove_double_space(self, texts):\n",
    "        print('Removing double space...')\n",
    "        pattern = re.compile(' +')\n",
    "        return [re.sub(pattern, ' ', text) for text in texts]\n",
    "        \n",
    "    def remove_punctuation(self, texts):\n",
    "        print('Removing Punctuation...')\n",
    "        pattern = re.compile('[^a-z ]')\n",
    "        return [re.sub(pattern, ' ', text) for text in texts]\n",
    "    \n",
    "    def remove_stopwords(self, texts):\n",
    "        print('Removing stopwords...')\n",
    "        return [[w for w in text.split(' ') if w not in self.stopwords] for text in tqdm(texts)]\n",
    "    \n",
    "    def remove_numbers(self, texts):\n",
    "        print('Removing numbers...')\n",
    "        return [' '.join([w for w in text if not w.isdigit()]) for text in tqdm(texts)]\n",
    "    \n",
    "    def decode_emojis(self, texts):\n",
    "        print('Decoding emojis...')\n",
    "        return [emoji.demojize(text, language='en') for text in texts] \n",
    "    \n",
    "    def lemmatize(self, texts):\n",
    "        print('Lemmatizing...')\n",
    "        lemmatized_texts = []\n",
    "        for text in tqdm(texts):\n",
    "            doc = nlp(text)\n",
    "            lemmatized_texts.append(' '.join([token.lemma_ for token in doc]))\n",
    "                                    \n",
    "        return lemmatized_texts\n",
    "        \n",
    "    def transform(self, X, y=None, mode='train'):\n",
    "        X = X.copy()\n",
    "        print('Removing Nans...')\n",
    "        X = X[~X.isnull()]                          # delete nans\n",
    "        X = X[~X.duplicated()]                      # delete duplicates\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.train_idx = X.index\n",
    "        else:\n",
    "            self.test_idx = X.index\n",
    "        print('Counting capitalized...')\n",
    "        capitalized = [np.sum([t.isupper() for t in text.split()]) \n",
    "                           for text in np.array(X.values)]  # count capitalized\n",
    "        # X['cap'] = capitalized\n",
    "        print('Lowering...')\n",
    "        X = [text.lower() for text in X]             # lower\n",
    "        X = self.remove_urls(X)                      # remove urls\n",
    "        X = self.remove_punctuation(X)               # remove punctuation\n",
    "        X = self.remove_double_space(X)              # remove double space\n",
    "        X = self.decode_emojis(X)                    # decode emojis\n",
    "        X = self.remove_stopwords(X)                 # remove stopwords\n",
    "        X = self.remove_numbers(X)                   # remove numbers                      \n",
    "        X = self.lemmatize(X)                        # lemmatize\n",
    "        \n",
    "        if not self.vectorizer_fitted:\n",
    "            self.vectorizer_fitted = True\n",
    "            print('Fitting vectorizer...')\n",
    "            self.vectorizer.fit(X)\n",
    "\n",
    "        print('Vectorizing...')\n",
    "        X = self.vectorizer.transform(X)             # vectorize\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr=Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=final_dataset\n",
    "y_final=final_dataset.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pr = pr.transform(data_train['text'])\n",
    "data_train_pr = pd.DataFrame.sparse.from_spmatrix(data_train_pr, columns=pr.vectorizer.get_feature_names_out())\n",
    "y_final = y_final[y_final.index.isin(pr.train_idx)]\n",
    "y_final.index = data_train_pr.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_train_pr.pkl','wb') as p:\n",
    "    pickle.dump(data_train_pr,p)\n",
    "\n",
    "with open('y_final.pkl','wb') as p:\n",
    "    pickle.dump(y_final,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data_train_pr.pkl','rb') as p:\n",
    "    X_data=pickle.load(p)\n",
    "with open('y_final.pkl','rb') as p:\n",
    "    y_data=pickle.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_data,y_data,test_size=0.4,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Logistic regression </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from scipy.stats import uniform, randint\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "# from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_cv(model, X_train, y_train, params, n_splits=5, scoring='f1_weighted'):\n",
    "#     kf = KFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "\n",
    "#     cv = RandomizedSearchCV(model,\n",
    "#                         params,\n",
    "#                         cv=kf,\n",
    "#                         scoring=scoring,\n",
    "#                         return_train_score=True,\n",
    "#                         n_jobs=-1,\n",
    "#                         verbose=2,\n",
    "#                         random_state=1\n",
    "#                         )\n",
    "#     cv.fit(X_train, y_train)\n",
    "\n",
    "#     print('Best params', cv.best_params_)\n",
    "#     return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs_parameters = {\n",
    "#     'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "#     'C': uniform(scale=10),\n",
    "#     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
    "#     'l1_ratio': uniform(scale=10)\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "# model_cv_lr = train_cv(lr, X_train, y_train, rs_parameters)\n",
    "\n",
    "# bestimator_lr = model_cv_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, bestimator_lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# sns.heatmap(confusion_matrix(y_test, bestimator_lr.predict(X_test)), annot=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pytorch Lightning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.nn import functional as F\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import random_split\n",
    "# import pytorch_lightning as pl\n",
    "# import torch.utils.data as data_utils\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "# from IPython.display import clear_output\n",
    "# import numpy as np\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# def plot_loss(losses, title=''):\n",
    "#     \"\"\"\n",
    "#     plots and saves loss progression while fitting\n",
    "#     \"\"\"\n",
    "#     if len(losses) < 3:\n",
    "#         return\n",
    "\n",
    "#     pos = np.vstack(losses)\n",
    "#     x, y = pos.T\n",
    "#     plt.clf()\n",
    "#     plt.ion()\n",
    "#     plt.figure(figsize=(9, 5))\n",
    "#     plt.plot(x, y)\n",
    "#     plt.title(title)\n",
    "#     clear_output(wait=True)\n",
    "#     plt.show()\n",
    "\n",
    "# class NNSentimentClassifier(pl.LightningModule):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "#         self.model = nn.Sequential(\n",
    "#         nn.Linear(8000, 1000),\n",
    "#         nn.ReLU(),\n",
    "#         self.dropout,\n",
    "#         nn.Linear(1000, 100),\n",
    "#         nn.Tanh(),\n",
    "#         self.dropout,\n",
    "#         nn.Linear(100, 1000),\n",
    "#         nn.ReLU(),\n",
    "#         self.dropout,\n",
    "#         nn.Linear(1000, 10),\n",
    "#         nn.ReLU(),\n",
    "#         self.dropout,\n",
    "#         nn.Linear(10, 4)\n",
    "#         )\n",
    "#         self.acc_train_loss = []\n",
    "#         self.acc_val_loss = []\n",
    "    \n",
    "#     def forward (self, x):\n",
    "#         prediction = self.model(x)\n",
    "#         return prediction\n",
    "        \n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "#         scheduler = StepLR(optimizer, step_size=3, gamma=0.1, verbose=True)\n",
    "#         return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "#     def training_step(self, train_batch, batch_idx):\n",
    "#         x, y = train_batch\n",
    "#         y = y.long()\n",
    "#         prediction = self.forward(x.float())\n",
    "#         loss = F.cross_entropy(prediction, y)\n",
    "#         self.log('train_loss', loss)\n",
    "#         return loss\n",
    "    \n",
    "#     def validation_step(self, val_batch, batch_idx):\n",
    "#         x, y = val_batch\n",
    "#         y = y.long()\n",
    "#         prediction = self.forward(x.float())\n",
    "#         loss = F.cross_entropy(prediction, y)\n",
    "#         self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "    \n",
    "#     def test_step(self, test_batch, batc_idx):\n",
    "#         x, y = test_batch\n",
    "#         y = y.long()\n",
    "#         prediction = model.forward(x.float())\n",
    "#         preds = torch.argmax(prediction, dim=1)\n",
    "\n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data\n",
    "# target_to_idx = {\n",
    "#     'Irrelevant': 0,\n",
    "#     'Negative': 1,\n",
    "#     'Neutral': 2,\n",
    "#     'Positive': 3\n",
    "#     }\n",
    "\n",
    "\n",
    "# x_tensor=torch.from_numpy(X_train.to_numpy().astype(np.float64))\n",
    "# y_tensor = torch.from_numpy(y_train.map(target_to_idx).values.astype(np.float64))\n",
    "# train_data_tensor = data_utils.TensorDataset(x_tensor, y_tensor)\n",
    "# val_size = int(len(train_data_tensor)*0.2)\n",
    "# train_size = len(train_data_tensor)- int(len(train_data_tensor)*0.2)\n",
    "# train_data_tensor, val_data_tensor = random_split(train_data_tensor, [train_size, val_size])\n",
    "# train_loader = DataLoader(train_data_tensor, batch_size=10,num_workers=5)\n",
    "# val_loader = DataLoader(val_data_tensor, batch_size=1,num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model\n",
    "# model = NNSentimentClassifier()\n",
    "# # training\n",
    "\n",
    "# trainer = pl.Trainer(gpus=1, precision=16, limit_train_batches=0.5, max_epochs=50)\n",
    "# trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_idx = torch.from_numpy(y_test.map(target_to_idx).values.astype(np.float64))\n",
    "# test_data_tensor = data_utils.TensorDataset(torch.from_numpy(X_test.to_numpy().astype(np.float64)), y_test_idx)\n",
    "# test_loader = DataLoader(test_data_tensor, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LSTM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_size = 32\n",
    "epochs=20\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "model= Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=8000))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(32)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfa8da227f197b2c550201a866dac7bf581188a13abdeff9264a381f9a8fd519"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
