{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DATA PREPROCESSING</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>EDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as  plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset=pd.read_csv('final_dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw_list=stopwords.words('english')\n",
    "word_count={\n",
    "    \"positive\":[],\n",
    "    \"negative\":[],\n",
    "    \"neutral\":[],\n",
    "    \"irrelevant\":[]\n",
    "}\n",
    "\n",
    "pattern=re.compile(\"[^\\w ]\")\n",
    "\n",
    "for text,target in zip(final_dataset.text,final_dataset.sentiment):\n",
    "    text=re.sub(pattern,\"\",text).lower().split()\n",
    "    text=[word for word in text if word not in sw_list]\n",
    "    word_count[target].extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20,10.5))\n",
    "for axis, (target, words) in zip(axes.flatten(), word_count.items()):\n",
    "    bar_info = pd.Series(words).value_counts()[:25]\n",
    "    sns.barplot(x=bar_info.values, y=bar_info.index, ax=axis)\n",
    "    axis.set_title(f'Top words for {target}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PREPROCESSING</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacy.lang.en import English\n",
    "import emoji\n",
    "import spacy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, stopwords=stopwords):\n",
    "        self.vectorizer = TfidfVectorizer(lowercase=False, max_features=8000,\n",
    "                                         min_df=10, ngram_range=(1, 3),\n",
    "                                         tokenizer=None)\n",
    "        self.stopwords = stopwords\n",
    "        self.vectorizer_fitted = False\n",
    "        \n",
    "    def remove_urls(self, texts):\n",
    "        print('Removing URLs...')\n",
    "        pattern = re.compile('(\\w+\\.com ?/ ?.+)|(http\\S+)')\n",
    "        return [re.sub(pattern, '', text) for text in texts]\n",
    "    \n",
    "    def remove_double_space(self, texts):\n",
    "        print('Removing double space...')\n",
    "        pattern = re.compile(' +')\n",
    "        return [re.sub(pattern, ' ', text) for text in texts]\n",
    "        \n",
    "    def remove_punctuation(self, texts):\n",
    "        print('Removing Punctuation...')\n",
    "        pattern = re.compile('[^a-z ]')\n",
    "        return [re.sub(pattern, ' ', text) for text in texts]\n",
    "    \n",
    "    def remove_stopwords(self, texts):\n",
    "        print('Removing stopwords...')\n",
    "        return [[w for w in text.split(' ') if w not in self.stopwords] for text in tqdm(texts)]\n",
    "    \n",
    "    def remove_numbers(self, texts):\n",
    "        print('Removing numbers...')\n",
    "        return [' '.join([w for w in text if not w.isdigit()]) for text in tqdm(texts)]\n",
    "    \n",
    "    def decode_emojis(self, texts):\n",
    "        print('Decoding emojis...')\n",
    "        return [emoji.demojize(text, language='en') for text in texts] \n",
    "    \n",
    "    def lemmatize(self, texts):\n",
    "        print('Lemmatizing...')\n",
    "        lemmatized_texts = []\n",
    "        for text in tqdm(texts):\n",
    "            doc = nlp(text)\n",
    "            lemmatized_texts.append(' '.join([token.lemma_ for token in doc]))\n",
    "                                    \n",
    "        return lemmatized_texts\n",
    "        \n",
    "    def transform(self, X, y=None, mode='train'):\n",
    "        X = X.copy()\n",
    "        print('Removing Nans...')\n",
    "        X = X[~X.isnull()]                          # delete nans\n",
    "        X = X[~X.duplicated()]                      # delete duplicates\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.train_idx = X.index\n",
    "        else:\n",
    "            self.test_idx = X.index\n",
    "        print('Counting capitalized...')\n",
    "        capitalized = [np.sum([t.isupper() for t in text.split()]) \n",
    "                           for text in np.array(X.values)]  # count capitalized\n",
    "        # X['cap'] = capitalized\n",
    "        print('Lowering...')\n",
    "        X = [text.lower() for text in X]             # lower\n",
    "        X = self.remove_urls(X)                      # remove urls\n",
    "        X = self.remove_punctuation(X)               # remove punctuation\n",
    "        X = self.remove_double_space(X)              # remove double space\n",
    "        X = self.decode_emojis(X)                    # decode emojis\n",
    "        X = self.remove_stopwords(X)                 # remove stopwords\n",
    "        X = self.remove_numbers(X)                   # remove numbers                      \n",
    "        X = self.lemmatize(X)                        # lemmatize\n",
    "        \n",
    "        if not self.vectorizer_fitted:\n",
    "            self.vectorizer_fitted = True\n",
    "            print('Fitting vectorizer...')\n",
    "            self.vectorizer.fit(X)\n",
    "\n",
    "        print('Vectorizing...')\n",
    "        X = self.vectorizer.transform(X)             # vectorize\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr=Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=final_dataset\n",
    "y_train=final_dataset.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pr = pr.transform(data_train['text'])\n",
    "data_train_pr = pd.DataFrame.sparse.from_spmatrix(data_train_pr, columns=pr.vectorizer.get_feature_names_out())\n",
    "y_train = y_train[y_train.index.isin(pr.train_idx)]\n",
    "y_train.index = data_train_pr.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv(\"ytest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfa8da227f197b2c550201a866dac7bf581188a13abdeff9264a381f9a8fd519"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
